TODO

* 比较 RVV 和 SIMD，RVV 设计哲学

* 寻找算子库

一些 paper

* [Exploring the Design Space of SPMD Divergence Management on Data-Parallel Architectures](https://d1qx31qr3h6wln.cloudfront.net/publications/MICRO_2014_Predication.pdf)
* [In-Datacenter Performance Analysis of a Tensor Processing Unit TM](https://arxiv.org/pdf/1704.04760)

## RVV Design

[Vector Extension Proposal](https://riscv.org/wp-content/uploads/2015/06/riscv-vector-workshop-june2015.pdf)

### Design Goal

* Support both implicit auto-vectorizaTon (OpenMP) and explicit SPMD (OpenCL) programming models
* Work with virtualization layers
* ~~Efficient and scalable to all reasonable design points~~
  * low-cost or high-performance
  * In-order, decoupled, or out-of-order microarchitectures
  * Integer, fixed-point, and/or floating-point data types

undefined -> out of order ，unmodified --> in order 

Features

* ~~Reconfigurable vector registers~~
* Polymorphic Instruction Encoding
* Mixed-precision support
* Unit-stride, Strided, Indexed Load/Stores
* Predication

### Vector versus Packed-SIMD versus GPU

* stripmine: no tail loop, no extra scalar operation

## Linear Algebra and AI Library

面向领域的大小？

### [AMD: Rocm](https://rocm.docs.amd.com/en/latest/what-is-rocm.html)

[HIP 编程接口](https://github.com/ROCm/HIP)

* [AMD rocBLAS](https://github.com/ROCm/rocBLAS)
* 

### CUDA

* [cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html)，闭源
* [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/latest/api/overview.html)，闭源，实现了例如卷积等的深度神经网络常用的算子，作为机器学习框架的 building blocks，一个用它实现卷积的 [example](https://www.goldsborough.me/cuda/ml/cudnn/c++/2017/10/01/14-37-23-convolutions_with_cudnn/)
* [cutlass](https://github.com/NVIDIA/cutlass)，实现高性能 GEMM 的算子组件，可用于开发定制其它机器学习相关的算子

* [spconv](https://github.com/traveller59/spconv)，稀疏卷积算子
* [TensorRT](https://github.com/NVIDIA/TensorRT)，部分开源（插件和解析器），优化神经网络的部署（计算图调整，精度调整等），黑盒 

## 文献调研

### MICRO 2023

UNICO: Unified Hardware Software Co-Optimization for Robust Neural Network Acceleration

Spatula: A Hardware Accelerator for Sparse Matrix Factorization

AuRORA: Virtualized Accelerator Orchestration for Multi-Tenant Workloads



Eureka: Efficient Tensor Cores for One-sided Unstructured Sparsity in DNN Inference

Cambricon-R: A Fully Fused Accelerator for Real-Time Learning of Neural Scene Representation

A Tensor Marshaling Unit for Sparse Tensor Algebra on General-Purpose Processors

Point Cloud Acceleration by Exploiting Geometric Similarity

> These applications interact with people in real-time on edge devices and thus require low latency and low energy. To accelerate the execution of deep neural networks (DNNs) on point clouds, some customized accelerators have been proposed, which achieved a significantly higher performance with reduced energy consumption than GPUs and existing DNN accelerators.

MAICC : A Lightweight Many-core Architecture with In-Cache Computing for Multi-DNN Parallel Inference

> To solve the above problem, we design an area- and energy-efficient manycore architecture by integrating large amounts of lightweight processor cores with RV32IMA ISA. The architecture leverages the emerging SRAM-based computing-in-memory technology to implement vector instruction extensions by reusing memory cells in the data cache instead of conventional logic circuits.

### MICRO 2022

Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design

DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation

> DFX has a flexible and custom instruction set architecture (ISA) at the assembly language level to support the end-toend processing for GPT-2 inference, unlike previous NLP accelerators that primarily focus on attention

### MICRO 2021

PointAcc: Efficient Point Cloud Accelerator

Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture

Capstan: A Vector RDA for Sparsity

> This paper proposes Capstan: a scalable, parallel-patterns-based, reconfigurable dataflow accelerator (RDA) for sparse and dense tensor applications. 

Graph processing, 3d vision acceleration

### HPCA 2023

SGCN: Exploiting Compressed-Sparse Features in Deep Graph Convolutional Network Accelerators

GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks

algorithm, hardware co-design

VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs

> This work presents VEGETA, a set of ISA and microarchitecture extensions over dense matrix engines to support flexible structured sparsity for CPUs, enabling programmable support for diverse DL models with varying degrees of sparsity

FlowGNN: A Dataflow Architecture for Real-Time Workload-Agnostic Graph Neural Network Inference

Mix-GEMM: An efficient HW-SW Architecture for Mixed-Precision Quantized Deep Neural Networks Inference on Edge Devices

### HPCA 2022

Direct Spatial Implementation of Sparse Matrix Multipliers for Reservoir Computing

Parallem Time Batching: Systolic-Array Acceleration of Sparse Spiking Nerual Computation

ReGNN: A Redundancy-Eliminated Graph Neural Networks Accelerator

S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration

Adaptable Register File Organization for Vector Processors

Griffin: Rethinking Sparse Optimization for Deep Learning Architectures

### HPCA 2021

Heterogeneous Dataflow Accelerators for Multi-DNN Workloads

SPAGHETTI: Streaming Accelerators for Highly Sparse GEMM on FPGAs

SpaceA: Sparse Matrix Vector Multiplication on Processing-in-Memory Accelerator

VIA: A Smart Scratchpad for Vector Units with Application to Sparse Matrix Computations

### FPGA 2024

HiSpMV: Hybrid Row Distribution and Vector Buffering for Imbalanced SpMV Acceleration on FPGAs

A Composable Dynamic Sparse Dataflow Architecture for Efficient Event-based Vision Processing on FPGA

> However, processing such dynamically sparseinput originated from event cameras efficiently in real time, par-ticularly with complex deep neural networks (DNN), remains aformidable challenge. Existing solutions that employ GPUs andother frame-based DNN accelerators often struggle to efficientlyprocess the dynamically sparse event data, missing the opportuni-ties to improve processing efficiency with sparse data. To addressthis, we propose ESDA, a composable dynamic sparse dataflowarchitecture that allows customized DNN accelerators to be con-structed rapidly on FPGAs for event-based vision tasks

## Unico(huawei)

感觉本文对标的是 HASCO, FAST 这类的 soft hard co-design space 的

另外一些  SW mapping exploration tool 包括：[FlexTensor](https://github.com/pku-liang/FlexTensor)，[GAMMA](https://github.com/maestro-project/gamma)

在 HW, SW 都确定后，需要使用 Analytical cost models 估计 power-performance-area（PPA），这些 model 有 MAE-STRO，TimeLoop。这些不是 cycle-accurate models，相对速度更快一些，而 Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing 这种是 CAModels

提到了 open-source micro-architectural accelerator model MAESTRO，用来做 PPA 估计

在每一次迭代优化中，选择 N 个 Hardware Configuration，进行 modified successive halving

文中提到了两个 DNN 加速器模板：

* HASCO: Towards Agile Hardware and Software CO-design for Tensor Computation

* Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing

感觉上加速器模板长这样：二维 PE 阵列，每个 PE 中包含 L1 buffer，整个 PE 包含 L2 buffer，所有可配置的参数包括二维阵列的大小，L1 和 L2 的大小，NOC bandwidth

hardware dataflow style：weight-stationary 和 output-stationary 有啥区别

实验部分：

* 比较 PPA，发现自己的更好，这不涉及物理实现

### Scratchpad Memory

加速器的每个 PE 里包含 Scratchpad Memory

参考论文 Scratchpad Memory : A Design Alternative for Cache On-chip memory in Embedded Systems

相比于 Cache：**ISA 层面显式存在的一块 memory，不与 main memory 同步，不需要 miss / hit 逻辑以及 tag array，由 application 和 compiler 显式管理**

### Multi-Objective Bayesian Optimization

多目标的贝叶斯优化？论文中引用的这一篇 HyperMapper: a Practical Design Space Exploration Framework. 

一些涉及到的关键词：surrogate model，Gaussian Process，

## Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing (Industry Track)

对 DNN 加速器的分类：training vs inference，cloud vs edge vs embedded ?

> Although an optimized accelerator can be proposed for each application/field, it is impractical to implement each of them, mainly due to the cost consideration. First, as the IC technology scales down, the chip fabrication expense increases exponentially. In addition, most DNN algorithms are developed with several well-known frameworks, such as PyTorch [16], TensorFlow [20], etc. And the deployment of these DNN models on a hardware also highly relies on intermediate software tool-chains, such as compilers and computing libraries [6, 19]. However, the development of these tools can be more time-consuming and cost-consuming than the hardware implementation itself

> Consequently, it is always a critical choice to trade off between the efficiency and the generality before implementing a DNN accelerator. Our company keeps focus on the vision of enabling ubiquitous AI in an all connected intelligent world. As listed in Table 1, the DNN accelerator should drive applications ranging from smart watches, smart phones, smart cars to smart cloud. It is no doubt that a unified scalable architecture, which can be easily "tailored" to fit in different scenarios, is preferred.

配置参数：cube unit，vector unit 的数量，L0，L1，Unified buffer 的带宽和大小

Ascend-Max，Ascend-Lite，Ascend-Tiny

> Ascend cores have been integrated into many SoCs used in different domains. They include DNN inference and training devices in servers, mobile processors, autonomous driving systems, wireless base station processors, IoT devices, etc.

一些应用的例子：Ascend910 SoC for trainning

Related Work 中谈到的一些 AI accelerator 设计范式：GPGPU，Systolic Array，**Dataflow Architecture**，一些其它 specialized architectures。**不太理解它这个 Cube unit 属于哪种**

> In the actual design, we flat the 3D cube’s layout to 2D to arrange it on the silicon die.

### Misc

[Understanding Matrix Multiplication on a Weight-Stationary Systolic Architecture](https://www.telesens.co/2018/07/30/systolic-architectures/) 中讨论了 systolic array 实现矩阵乘法的基本结构，通过额外添加一个 accumulator 来实现更大矩阵的乘法。一些 systolic array 的缺点

* 当矩阵的维数不是 systolic array 的倍数时，systolic array 的利用率可能不高
* no support for sparsity



**一些还没明白的地方：这些加速器的指令集该长啥样？编译器是如何工作的？这些加速器是否支持 trainning？**

## FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System

第一个问题是 FlexTensor 这种 software mapping tools 和 AI Compiler 有什么区别。从 [How to Bring Your Own Codegen to TVM](https://tvm.apache.org/2020/07/15/how-to-bring-your-own-codegen-to-tvm) 中看来，AI Compiler 做的是更高层的算子级别的优化，而具体算子怎么在硬件上实现则是 FlexTensor 这类工具关心的问题

FlexTensor 在实验中考虑了不同的硬件平台：NVIDIA V100 GPU（与 cuDNN 比较），Intel Xeon CPU（与 MKL-DNN 比较），Xilinx VU9P FPGA（与 OpenCL baseline 相比较）

整个搜索算法的输出是一个 TVM 中的由 schedule primitive 组成的调度结果，具体的搜索方法采取的是模拟退火+DQN

由于 Q-Learning 和 DQN 参考了一下[从 Q-Learning 到 DQN](https://wulc.me/2018/05/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)-%E4%BB%8E%20Q-Learning%20%E5%88%B0%20DQN/)

在 FPGA 上做性能估计时考虑了 three stage pipeline 这种结构，没找到出处（感觉在参考论文 66 中）

在实验中一部分性能不如 cuDNN 的原因在于 cuDNN 中使用了其它的算法。提到的一些包括 Winograd algorithm， Fast Convolutional Nets With fbfft

> We can choose different orders to generate the schedule depending on the priority given to the graph and the nodes. Alternative orders are bottom-up order, top-down order and recurrent order. The bottom-up order generates a schedule for each node in a mini-graph first and then generates schedules for the whole graph. The top-down order is on the opposite. And the recurrent order is to repeat bottom-up order or topdown order for several times

5.3 节中这句话没看明白，主要对 TVM 不太懂

## EIE: Efficient Inference Engine on Compressed Deep Neural Network

EIE 主要面向的应用是 SPMV，**在  Flexibility 一节中声称可以使用 Winograd 算法将卷积转换为 MV，为什么？**

## Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks

如何在仅支持低维卷积的加速器上做高维卷积 --> 拆成低维的，分别计算卷积然后做累加

如何在仅支持小的卷积核上的加速器（因为 scratchpad memory 不够）做大的卷积核 --> 把大的卷积和拆成小的，分别计算卷积后累加

文中提到了卷积中的 data reuse：

* Convolutional Reuse：这是说因为卷积核在 input feature map 上移动，卷积核的 weight 被反复使用
* Filter Reuse：这是说由于 batch 处理，卷积核的 weight 在不同的 input feature map 上被重复用了
* Ifmap Reuse：这是说由于输出多个 channel，input feature map 的 pixel 在不同的 channel 对应的卷积核上运算时被重复使用了

文中使用了称为 Row Stationary 的 dataflow

* 如何处理 stride > 1 的情况：很简单嘛，首先 PE 内部做一维卷积的时候考虑到步长的因素，然后每个 column 需要考虑到步长，例如在 stride = 2，kernel = 3x3 时，col 1 是对应 input feature 的 row 1, 2, 3，而 col 2 是对应 input feature 的 row 3, 4, 5
* 如何处理  dilated conv：同样的，PE 内部支持一维的 dilated conv，然后每个 column 需要考虑 dilation，例如 dilation = 2 时，col 1 对应的 input feature 的 row 1, 3, 5

文中声称这个结构还可以用来做 FC 和 POOL，当然没问题，但这俩 layer 基本没有 data reuse，我很怀疑其性能

数据压缩和利用 sparsity：

* [Eyeriss](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7738524) 中存在一个 Runtime Length Compression（RLC）模块，它用来做 activation compression。根据统计 feature map 中很多数据都是 0，那么 Eyeriss 在读写 DRAM 时使用了压缩的格式（和 EIE 中使用的 CSC 很像）来减少读写的次数
* 关于 sparsity 是我的推测：如果 PE 读入 feature map 时也采用这种压缩的格式的话，我觉得对利用 sparsity 也很友好。另外 weight 为 0 时跳过计算是不是也算利用 sparsity（但没能节省带宽）
* 





### 待读的：

~~EIE: Efficient Inference Engine on Compressed Deep Neural Network，必读~~

~~Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks，必读~~

High Performance Convolutional Neural Networks for Document Processing，img2col 出处，必读

Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices

ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA, fpga 相关，感觉是 EIE 这篇的 fpga 实现

HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing，fpga 相关

SpWA: An Efficient Sparse Winograd Convolutional Neural Networks Accelerator on FPGAs，fpga 相关

Exploring Heterogeneous Algorithms for Accelerating Deep Convolutional Neural Networks on FPGAs，fpga 相关

Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network，fpga 相关

FBLAS: Streaming Linear Algebra on FPGA，fpga 相关

Cambricon: An Instruction Set Architecture for Neural Networks，2016 isca

Anatomy Of High-Performance Deep Learning Convolutions On SIMD Architectures，cpu 相关

Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines，与 AI 编译，TVM 有从中借鉴，没时间可以不读

### TinyML 的 Lec3 谈到的另外两篇

SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning

SpArch: Efficient Architecture for Sparse Matrix Multiplication

