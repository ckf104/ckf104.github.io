## 1. An Efficient Algorithm for Exploiting Multiple Arithmetic Units

这篇 1967 年的论文首次介绍了在 system/360 中使用的 tomasulo 算法来乱序执行指令，提高并行效率。我会分几个部分来讨论这篇论文

### 算法实现

tomasulo 算法的两大核心是 reservation station 和 common data bus。

reservation station 主要起到了两方面的作用，一个是提供了寄存器重命名中需要的名称，论文中称为 tag。它使得我们通过 tag 来追踪指令间的依赖，而不是通过逻辑寄存器的名称。因为 tag 的数目仅受限于硬件资源，而逻辑寄存器的名称还受限于指令编码，因此往往 tag 的数目可以远多于逻辑寄存器的数目，这极大地减少了 WAR 和 WAW 这类由于名称导致的冒险。reservation station 的另一个作用是提供了类似于 instruction buffer 的指令缓冲，这使得处理器中能够包含更多 in-flight 的指令，使得处理器能够看到更多的潜在并行性。

common data bus 则提供了数据广播的线路，因为在引入 reservation station 后，仅仅是将运算结果写回寄存器是不够的，因为保留站中的指令也可能依赖于该运算结果。因此 common data bus 这条广播线路便将运算结果和它的 tag 向所有保留站和寄存器堆进行广播，如果 tag 比对成功，那么保留站或者寄存器堆则保留该数据。

具体来说，算法相比于传统的顺序发射流水线，除了 cdb 这条广播线路外，为每个逻辑寄存器额外增加了 tag 和 busy 位域。并且为每个执行单元配备了一定数目的 reservation station。每个 reservation station 都有一个 tag 编号。接下来我们将看到 tag 表达了指令间的数据依赖关系。

每条指令在解码后，会根据它依赖的执行单元分配一个保留站给它。该指令根据源操作数读取逻辑寄存器时，如果该寄存器的 busy 值为 0，表明寄存器值有效，将值读取后存放在保留站中。如果 busy 值为 1，那么该寄存器的 tag 值有效，读取 tag 值存放在保留站中。然后更新目的寄存器的 busy 值为 1，tag 值为该指令分配到的保留站的编号。

当指令在保留站中时，如果操作数没有准备就绪，那么指令知道产生这个操作数的指令的保留站 tag。当 cdb 进行广播时，只需要比较 tag 就知道这是不是需要的操作数了。当操作数都准备就绪后，指令就可以进行执行单元进行执行了。

然后我们看写回阶段，指令在执行单元执行完毕后，需要广播它的结果和 tag 给所有保留站和寄存器堆。值得注意的是，该指令的结果并不总是会写回到寄存器堆中。假设指令 I1 需要写回寄存器 R1，后续有指令 I2 也会写回到寄存器 R1，并且 I2 在 I1 执行完毕前就发射到保留站中。那么在 I1 广播结果时，寄存器 R1 的 tag 域已经被 I2 修改为了 I2 的保留站编号，因此 I1 的结果不会被写入到 R1 中，这也正是我们想要的。当 I2 执行完毕，结果广播回寄存器 R1 时，此时 R1 将 busy 位置 0，表示寄存器中的数据是可读的最新值。

### Others

论文中还提到了一些该算法带来的额外好处以及挑战。

一个好处是寄存器重命名使得可以某些情况下可以 0 开销实现 mv 指令。考虑这样一种情况，假设 mv 将 R2 寄存器的值移动到 R1 寄存器，当 mv 读写寄存器堆时，R2 的值还未准备就绪，即 R2 的 busy 位为 1，那么该 mv 指令只需要把 R1 的 busy 位拉高，同时将 R1 的 tag 为设置为 R2 的 tag 即可。

但寄存器重命名没有办法处理部分读写寄存器带来的隐式依赖。论文中谈到，由于 system/360 中单精度浮点寄存器使用的是双精度浮点寄存器的低位。如果一条双精度浮点指令后面有一条单精度浮点指令，并且它们有 WAW 冒险，寄存器重命名是没有办法正确处理的，因为此时寄存器需要的结果来源于两条指令，双精度指令的结果需要写入寄存器的高位，单精度指令的结果写入寄存器的低位。论文中采取的办法是这种情况下简单地 stall 流水线（当然更加高效的办法可能是为每个寄存器和 reservation station 的操作数设置两个 tag，分别对应操作数的高位和低位）。现代的做法通常是在指令系统设计时规避这一点，例如从语义上要求单精度浮点指令会将寄存器高位置 0 等等。

## 2. The MIPS R10000 Superscalar Microprocessor

这里主要讨论 mips r10000 处理器的乱序设计，并与之和 tomasulo 算法进行一些比较。

tomasulo 算法其实是一种隐式的寄存器重命名，它将寄存器的名称映射到保留站的 tag 上，而每个逻辑寄存器的 tag 域则充当了映射表 mapping table。而每个逻辑寄存器的 busy 位则充当了 mips r10000 中的 busytable。在 mips r10000 中，这些结构都被显式地提出来作为了重命名阶段的模块。具体来讲，mips r10000 的重命名的功能由 mapping table，busytable，freelist 三部分组成。

mapping table 中保存了逻辑寄存器编号到物理寄存器编号的映射，在重命名阶段时，每条指令根据 mapping table，将自己需要读的逻辑寄存器编号转换为物理寄存器编号。busytable 中则保存了每个物理寄存器的 busy 位，如果为 1 则表明结果还未写回到该物理寄存器。freelist 中保存了空闲的物理寄存器（对应 tomasulo 算法中空闲的保留站）。需要该指令需要写回到逻辑寄存器中，那么就会从 freelist 中给该指令分配一个物理寄存器。并将该指令的逻辑寄存器编号与新分配的物理寄存器编号这个新的映射关系写入到 mapping table 中。

那么什么时候能判定一个物理寄存器是空闲的呢？mips r10000 中给出的答案是当指令 I 提交时，指令 I 写回的逻辑寄存器在指令 I 之前对应的物理寄存器变为空闲了。具体来说，假设指令 I 要写入逻辑寄存器 R1，它在指令 I 修改 mapping table 之前对应物理寄存器 P1。假设指令 I 分配到的物理寄存器是 P2，那么在指令 I 发射之后，R1 将会映射到 P2 上。那么当指令 I 被提交后，可以将 P1 加入到 freelist 中。那能不能更早一点，在指令 I 修改 mapping table 后，就将 P1 释放呢？毕竟在正常的指令流中，在指令 I 以后的指令不可能再访问物理寄存器 P1 了。如果考虑到精确中断和分支预测错误，那么答案是不行的。

实际上在 tomasulo 算法中并没有考虑两个方面：如果发生分支预测错误，并且误执行的指令修改了逻辑寄存器的 tag 域。又或者前面的指令发生了异常，为做到精确中断，需要排空流水线。这些情况下如何恢复逻辑寄存器原来的值和 tag 域。在指令 I 提交时才将更早的物理寄存器加入 freelist 也是出于这个考虑。如果更早地将物理寄存器 P1 释放，后续 P1 可能被其它指令使用，并且修改，那么当指令 I 前面的某条指令发生了异常，就没办法恢复我们需要的物理寄存器 P1 的值了。只有当指令 I 提交时，在 ISA 层面看到的是逻辑寄存器 R1 被指令 I 复写了，此时 P1 才能安全地释放。

为了在分支预测错误时恢复 freelist 和 mapping table，mips r10000 在每条分支指令发射时为 mapping table 和 freelist 创建了快照，在预测错误时使用快照在一个周期恢复 freelist 和 mapping table。而在发生异常时，则采取更慢的回滚方式。每个周期回滚 rob（论文中称为 active list）中的一条指令，将 freelist 和 mapping table 恢复到该指令执行之前的样子。因为异常发生的频率更低，因此回滚的延迟是可接受的。

显式引入物理寄存器堆并且在指令提交时才释放更早的物理寄存器使得精确中断能够在 mips r10000 上实现。

## 3. Decoupled Access/Execute Computer Architectures 

这篇论文的提出了一种 access 与 execute 分离的处理器架构，相比于 tomasulo 算法，能以更简单的方式实现乱序发射和执行。

总的来说，这种架构将处理器分为两部分，A processor 和 X processor，前者拥有 address register，处理地址计算和访存。后者拥有 data register，处理实际的计算。两个 processor 都是顺序发射，通过 queue 来进行数据交互。queue 包括

* XLQ：A processor 执行 load 指令得到的数据如果是用于 X processor 使用，那将送往 XLQ 的队头。X processor 中的指令如果源操作数之一是 XLQ，那么从 XLQ 的队尾弹出该数据，用于该指令的执行（如果此时 XLQ 为空，那么暂停流水线，禁止该指令发射）。
* A / X copy queue：用于 A processor 和 X processor 之间的寄存器数据交互。如果有 A processor 的寄存器数据送往 X processor，那么 A processor 中会有一条指令负责将 A processor 中的数据送往 A copy queue，而在 X processor 中会有一条指令负责弹出 A copy queue 队尾的数据，然后送往相应的寄存器。将 X processor 中的数据送往 A processor 的过程是类似的。
* XSQ，SAQ：这两个 queue 分别用于存储 store 指令的数据和地址。为完成一条 store 指令，在 X processor 中会有条指令计算 store 需要的数据，并送往 XSQ。而在 A processor 中则会有一条指令计算 store 的地址，送往 SAQ，当地址和数据都具备后就可以将向内存发起 store 请求了。
* ABQ / XBQ：这两个 queue 负责交互分支指令的结果。一条分支指令可能使用 X register 或者 A register 来判断是否跳转。如果使用 X register，那么在 X processor 中有一条分支指令，它会将分支指令的结果（是否跳转，跳转目标）送往 ABQ，同时 A processor 中会有一条 BFQ 指令，该指令弹出 ABQ 队尾的一个表项，根据表项中的数据选择如何跳转。如果分支指令使用 A register，处理也是类似的。

这种处理器架构有许多好处，例如实现了 access 和 execute 的解耦，使得每一部分的设计变得简单。并且也提供了一定程度的乱序发射能力，相比于 tomasulo 算法，硬件复杂度更低。但这种架构也存在以下的显著的问题

* X processor 的内存操作数只能从 XLQ 中获取，而 XLQ 中的数据只能读取一次。这意味着，如果我希望多次使用 load 上来的数据，需要额外执行一条 mv 指令，将 XLQ 中的数据搬运到 X register 中。

* X processor 与 A processor 同步较为麻烦。除了 branch 指令需要通过 ABQ / XBQ 进行同步外，异常和中断也需要同步。为了实现精确异常，我们需要为 X processor 和 A processor 都添加 rob，并且使用额外的 queue 通信，告知对方发生了异常。
* queue 是体系结构的一部分，而不是微体系结构的一部分。显然，queue 的内容是进程上下文的一部分，这意味着，queue 的长度寄存器，头尾指针寄存器等等是需要定义在 ISA 中的。另外，两个分别的指令流可能产生 deadlock，而一个给定的指令流是否产生 deadlock，是依赖于 queue 的长度的（queue 越长，越不容易产生 deadlock）。